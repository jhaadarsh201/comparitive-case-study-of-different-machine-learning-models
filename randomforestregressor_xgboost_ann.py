# -*- coding: utf-8 -*-
"""RandomForestRegressor | XgBoost | ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ON4xXhfM4jkFOWCJzS0A2t9hQvYEZ3IP
"""

!pip install ucimlrepo -q

from ucimlrepo import fetch_ucirepo

# fetch dataset
heart_disease = fetch_ucirepo(id=45)

df = heart_disease.data.original

df

df = df.fillna(df.mode().iloc[0])

target = 'num'
X = df[[col for col in df.columns if col != target]]
y = df[target]

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA
n_components = 7 # Number of principal components to retain
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Initialize and train Random Forest Regressor
rf_regressor = RandomForestClassifier(n_estimators=1000, random_state=42)
rf_regressor.fit(X_train_pca, y_train)

# Make predictions on the testing set
y_pred = rf_regressor.predict(X_test_pca)

# Evaluate the model
mse = accuracy_score(y_test, y_pred)
print("Accuracy", mse)

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df[target].unique(), yticklabels=df[target].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Initialize and train Random Forest Regressor
rf_regressor = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42)
rf_regressor.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = rf_regressor.predict(X_train)

# Evaluate the model
mse = accuracy_score(y_train, y_pred)
print("Mean Squared Error:", mse)

# Compute confusion matrix
cm = confusion_matrix(y_train, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df[target].unique(), yticklabels=df[target].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

#70% accuracy

"""# XgBoost"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import accuracy_score


# Initialize XGBoost classifier for multiclass classification
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), random_state=42)

# Train the model
xgb_classifier.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = xgb_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df[target].unique(), yticklabels=df[target].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# Upsampling"""

from imblearn.over_sampling import RandomOverSampler
import pandas as pd

ros = RandomOverSampler(random_state=42)

X_resampled, y_resampled = ros.fit_resample(X, y)

# Convert back to DataFrame
df_resampled = pd.DataFrame(X_resampled, columns=[col for col in df.columns if col != target])
df_resampled['num'] = y_resampled

df_resampled

target = 'num'
X = df_resampled[[col for col in df.columns if col != target]]
y = df_resampled[target]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize XGBoost classifier for multiclass classification
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), random_state=42)

# Train the model
xgb_classifier.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = xgb_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df[target].unique(), yticklabels=df[target].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# ANN"""



y.value_counts()

y.shape

y



import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Assuming y has shape (820,)
# Reshape y into a 2D array
y = np.array(y).reshape(-1, 1)

# One-hot encode the target variable
encoder = OneHotEncoder(sparse=False)
y_encoded = encoder.fit_transform(y)

y_encoded

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Define the architecture of the neural network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, input_shape=(X_train.shape[1],), activation='relu'), # Input layer with 10 neurons and ReLU activation
    tf.keras.layers.Dense(8, activation='relu'),  # Hidden layer with 8 neurons and ReLU activation
    tf.keras.layers.Dense(5, activation='softmax')  # Output layer with 5 neurons (for 5 classes) and softmax activation
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Accuracy: {test_accuracy}')

import matplotlib.pyplot as plt

# Train the model and store the training history
history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_test, y_test))

# Plot training loss vs validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training accuracy vs validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

y_preds = model.predict(X_test)



y_pred

type(y_pred)

# Get the index of the maximum valued column for each row
max_indices = [np.argmax(row) for row in y_preds]

# Create a DataFrame with a single column representing the index of the maximum valued column for each row
result_df = pd.DataFrame(max_indices, columns=['num'])



type(result_df)



y_test = result_df['num'].values

y_test

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=df[target].unique(), yticklabels=df[target].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()



X1 = ["ID3","C4.5","CART(overfit)","KNN(w/o Upsampling)","KNN(w/ Upsampling)","Naive Bayes(w/o Upsampling)","Naive Bayes(w Upsampling)","Random Forest(w/o Upsampling)","Random Forest(w/ upsampling)","XgBoost(w/o Upsampling)","XgBoost(w/ upsampling)","ANN(w/ Upsampling)"]

Y1 = [0.9867,0.9801,1,0.5409,0.7317,0.3278,0.4878,0.5245,0.9545,0.4918,0.9268,0.4573]

len(X1)

len(Y1)

import matplotlib.pyplot as plt

X1 = ["ID3", "C4.5", "CAR(overfit)", "KNN(w/o Upsampling)", "KNN(w/ Upsampling)", "Naive Bayes(w/o Upsampling)", "Naive Bayes(w/ Upsampling)", "Random Forest(w/o Upsampling)", "Random Forest(w/ upsampling)", "XgBoost(w/o Upsampling)", "XgBoost(w/ upsampling)", "ANN(w/ Upsampling)"]

Y1 = [0.9867, 0.9801, 1, 0.5409, 0.7317, 0.3278, 0.4878, 0.5245, 0.9545, 0.4918, 0.9268, 0.4573]

plt.figure(figsize=(10, 6))
plt.bar(X1, Y1, color='purple')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Algorithms')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.ylim(0, 1)  # Set y-axis limits
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better visualization
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

import matplotlib.pyplot as plt

X1 = ["ID3", "C4.5", "CAR(overfit)", "KNN(w/o Upsampling)", "KNN(w/ Upsampling)", "Naive Bayes(w/o Upsampling)", "Naive Bayes(w/ Upsampling)", "Random Forest(w/o Upsampling)", "Random Forest(w/ upsampling)", "XgBoost(w/o Upsampling)", "XgBoost(w/ upsampling)", "ANN(w/ Upsampling)"]

Y1 = [0.9867, 0.9801, 1, 0.5409, 0.7317, 0.3278, 0.4878, 0.5245, 0.9545, 0.4918, 0.9268, 0.4573]

plt.figure(figsize=(8, 6))
plt.barh(X1, Y1, color='skyblue')
plt.xlabel('Accuracy')
plt.ylabel('Algorithms')
plt.title('Accuracy of Different Algorithms')
plt.xlim(0, 1)  # Set x-axis limits
plt.grid(axis='x', linestyle='--', alpha=0.7)  # Add gridlines for better visualization
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

import matplotlib.pyplot as plt

X1 = ["ID3", "C4.5", "CAR(overfit)", "KNN(w/o Upsampling)", "KNN(w/ Upsampling)", "Naive Bayes(w/o Upsampling)", "Naive Bayes(w/ Upsampling)", "Random Forest(w/o Upsampling)", "Random Forest(w/ upsampling)", "XgBoost(w/o Upsampling)", "XgBoost(w/ upsampling)", "ANN(w/ Upsampling)"]

Y1 = [0.9867, 0.9801, 1, 0.5409, 0.7317, 0.3278, 0.4878, 0.5245, 0.9545, 0.4918, 0.9268, 0.4573]

plt.figure(figsize=(10, 6))
plt.plot(X1, Y1, marker='o', linestyle='-')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Algorithms')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better visualization
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

